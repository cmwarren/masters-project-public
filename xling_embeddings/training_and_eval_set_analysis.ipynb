{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lang_names = {\n",
    "#     'ar':'Arabic',\n",
    "#     'cs':'Czech',\n",
    "#     'de':'German',\n",
    "#     'el':'Greek',\n",
    "#     'en':'English',\n",
    "#     'es':'Spanish',\n",
    "#     'et':'Estonian',\n",
    "#     'fa':'Farsi',\n",
    "#     'fi':'Finnish',\n",
    "#     'fr':'French',\n",
    "#     'he':'Hebrew',\n",
    "#     'hi':'Hindi',\n",
    "#     'hu':'Hungarian',\n",
    "#     'id':'Indonesian',\n",
    "#     'is':'Icelandic',\n",
    "#     'it':'Italian',\n",
    "#     'ja':'Japanese',\n",
    "#     'ko':'Korean',\n",
    "#     'lt':'Lithuanian',\n",
    "#     'mi':'Maori',\n",
    "#     'my':'Myanmar',\n",
    "#     'ne':'Nepali',\n",
    "#     'pl':'Polish',\n",
    "#     'pt':'Portuguese',\n",
    "#     'ro':'Romanian',\n",
    "#     'ru':'Russian',\n",
    "#     'sk':'Slovak',\n",
    "#     'sv':'Swedish',\n",
    "#     'th':'Thai',\n",
    "#     'tr':'Turkish',\n",
    "#     'vi':'Vietnamese',\n",
    "#     'zh':'Chinese',\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lang_names = {\n",
    "    'af':'Afrikaans',\n",
    "    'ar':'Arabic',\n",
    "    'bg':'Bulgarian',\n",
    "    'cb':'Cebuano',\n",
    "    'cf':'Haitian Creole',\n",
    "    'cs':'Czech',\n",
    "    'da':'Danish',\n",
    "    'de':'German',\n",
    "    'el':'Greek',\n",
    "    'en':'English',\n",
    "    'eo':'Esperanto',\n",
    "    'es':'Spanish',\n",
    "    'et':'Estonian',\n",
    "    'fa':'Farsi',\n",
    "    'fi':'Finnish',\n",
    "    'fr':'French',\n",
    "    'he':'Hebrew',\n",
    "    'hi':'Hindi',\n",
    "    'hr':'Croatian',\n",
    "    'hu':'Hungarian',\n",
    "    'id':'Indonesian',\n",
    "    'is':'Icelandic',\n",
    "    'it':'Italian',\n",
    "    'ja':'Japanese',\n",
    "    'kn':'Kannada',\n",
    "    'ko':'Korean',\n",
    "    'la':'Latin',\n",
    "    'lt':'Lithuanian',\n",
    "    'lv':'Latvian',\n",
    "    'mg':'Malagasy',\n",
    "    'mi':'Maori',\n",
    "    'ml':'Malayalam',\n",
    "    'mr':'Marathi',\n",
    "    'my':'Myanmar',\n",
    "    'ne':'Nepali',\n",
    "    'nl':'Dutch',\n",
    "    'no':'Norwegian',\n",
    "    'pa':'Paite (Chin)',\n",
    "    'pl':'Polish',\n",
    "    'pt':'Portuguese',\n",
    "    'qe':'Q’eqchi’',\n",
    "    'ro':'Romanian',\n",
    "    'ru':'Russian',\n",
    "    'sk':'Slovak',\n",
    "    'sl':'Slovene',\n",
    "    'so':'Somali',\n",
    "    'sq':'Albanian',\n",
    "    'sr':'Serbian',\n",
    "    'sv':'Swedish',\n",
    "    'te':'Telugu',\n",
    "    'th':'Thai',\n",
    "    'tl':'Tagalog',\n",
    "    'tr':'Turkish',\n",
    "    'vi':'Vietnamese',\n",
    "    'ww':'English',\n",
    "    'xh':'Xhosa',\n",
    "    'zh':'Chinese',\n",
    "    'zm':'Zarma'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Corpus Analysis ###\n",
    "\n",
    "Investigate the ratio of word count to vocabulary size for each language (indicator of Isolating vs Polysynthetic qualities of the language).\n",
    "\n",
    "What proportion of the overall vocabulary is above the frequency threshold to be included in the embeddings.\n",
    "\n",
    "Contrast the two corpora in terms of size and breadth of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def corpus_lang_counts(dir_path, lang_counts=None):\n",
    "    \n",
    "    if lang_counts is None:\n",
    "        lang_counts = dict()\n",
    "        \n",
    "    for path in [join(dir_path, f) for f in listdir(dir_path) if f.endswith('.txt')]:\n",
    "        language_prefix = path[-7:-5]\n",
    "        with codecs.open(path, encoding=\"utf8\", errors=\"replace\") as fin:\n",
    "            lines = [line.strip() for line in fin]\n",
    "            vocab = Counter()\n",
    "            word_count = 0\n",
    "            for line in lines:\n",
    "                words = line.split('\\t')[1].split(' ')\n",
    "                word_count += len(words)\n",
    "                vocab.update(Counter(words))\n",
    "        lang_counts[language_prefix] = [len(lines), word_count, len(vocab.keys()), vocab]\n",
    "    \n",
    "    return lang_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get the word count and vocabulary size for each language file\n",
    "def corpus_counts(training_corpus, enrich_corpus=None):\n",
    "    dir_path = './' + training_corpus\n",
    "    lang_counts = corpus_lang_counts(dir_path)\n",
    "    if enrich_corpus is None:\n",
    "        vecs_path = dir_path + '/sgns'\n",
    "    else:\n",
    "        dir_path = './' + enrich_corpus\n",
    "        lang_counts = corpus_lang_counts(dir_path, lang_counts)\n",
    "        vecs_path = './' + training_corpus + '-' + enrich_corpus + '/sgns'\n",
    "    \n",
    "    # Get the size of the learnt vocabulary (thresholded at word_freq > 2)\n",
    "    for path in [join(vecs_path, f) for f in listdir(vecs_path) if f.endswith('.vecs.vocab')]:\n",
    "        language_prefix = path[-14:-12]\n",
    "        learnt = set()\n",
    "        if not is_number(language_prefix): # Exclude some extraneous files e.g. 741.vecs.vocab\n",
    "            count_list = lang_counts[language_prefix]\n",
    "            with codecs.open(path, encoding=\"utf-8\", errors=\"replace\") as fin:\n",
    "                lines = [line.strip() for line in fin]\n",
    "                for word in lines:\n",
    "                    learnt.add(word)\n",
    "                count_list.append(len(lines))\n",
    "                count_list.append(learnt)                \n",
    "        \n",
    "    return lang_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tabulate_corpus_counts(counter):\n",
    "    counts = dict()\n",
    "    for key,values in counter.items():\n",
    "        if key in lang_names:\n",
    "            counts[key] = ((key, codecs.decode(lang_names[key], 'utf-8', 'replace'), values[0], values[1], values[2], values[4]))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_corpus_counts(counts, enriched_counts=None):\n",
    "    print(str(len(counts)) + ' languages:\\n')\n",
    "    if enriched_counts is None:\n",
    "        print('lang\\tname\\t\\tsents\\twords\\tvocab\\tlearnt\\tlearnt%')\n",
    "        print('======\\t==========\\t======\\t======\\t======\\t======\\t======')\n",
    "        for key, lang in counts.items():\n",
    "            lang_name = lang[1] +'\\t' if len(lang[1])<8 else lang[1]\n",
    "            learnt_pc = round((lang[5]/lang[4])*100, 1)\n",
    "            print(lang[0] + '\\t' + lang_name + '\\t' + str(lang[2]) + '\\t' + str(lang[3]) + '\\t' + str(lang[4]) + \n",
    "                  '\\t' + str(lang[5]) + '\\t' + str(learnt_pc))\n",
    "    else:\n",
    "        print('\\t\\t\\t|enrich corpus\\t|unenriched\\t\\t|enriched\\t\\t|')\n",
    "        print('lang\\tname\\t\\t|sents\\twords\\t|vocab\\tlearnt\\tlearnt%\\t|vocab\\tlearnt\\tlearnt%\\t|learnt_gain%')\n",
    "        print('======\\t==========\\t|======\\t======\\t|======\\t======\\t======\\t|======\\t======\\t======\\t|============')\n",
    "        for key, lang in enriched_counts.items():\n",
    "            lang_name = lang[1] +'\\t' if len(lang[1])<8 else lang[1]\n",
    "            enriched_learnt_pc = round((lang[5]/lang[4])*100, 1)\n",
    "            learnt_pc = round((counts[key][5]/counts[key][4])*100, 1)\n",
    "            gain_pc = round((lang[5]/counts[key][5])*100, 1)\n",
    "            print(lang[0] + '\\t' + lang_name + '\\t' + '|' + str(lang[2]) + '\\t' + str(lang[3]) + '\\t' +\n",
    "                  '|' + str(counts[key][4]) + '\\t' + str(counts[key][5]) + '\\t' + str(learnt_pc) + '\\t' +\n",
    "                  '|' + str(lang[4]) + '\\t' + str(lang[5]) + '\\t' + str(enriched_learnt_pc) + '\\t' +\n",
    "                  '|' + str(gain_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bar_chart_corpus_counts(counts, corpus):\n",
    "    counts = counts.values()\n",
    "    N = len(counts)\n",
    "    \n",
    "    words = tuple([int(lang[3]) for lang in counts])\n",
    "\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.3       # the width of the bars\n",
    "    gap = 0.05\n",
    "\n",
    "    fig, ((ax1), (ax2)) = plt.subplots(nrows=2, ncols=1, sharex=True) # Create matplotlib figure\n",
    "    fig.set_size_inches(width*N*2 + gap*N, 16)\n",
    "    fig.suptitle(corpus + ' corpus')\n",
    "    \n",
    "    rects1 = ax1.bar(ind, words, width, color='r')\n",
    "\n",
    "    vocab = tuple([int(lang[4]) for lang in counts])\n",
    "    rects2 = ax2.bar(ind, vocab, width, color='b')\n",
    "    \n",
    "    learnt = tuple([int(lang[5]) for lang in counts])\n",
    "    rects3 = ax2.bar(ind + width, learnt, width, color='g')\n",
    "\n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax1.set_ylabel('Word count')\n",
    "    ax2.set_ylabel('Vocabulary size')\n",
    "    ax1.set_xticks(ind)\n",
    "    ax2.set_xticks(ind)\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.set_xticklabels(tuple([lang[1] for lang in counts]), rotation=45)\n",
    "    ax2.set_xticklabels(tuple([lang[1] for lang in counts]), rotation=45)\n",
    "\n",
    "    ax2.legend((rects2, rects3), ('Vocab', 'Learnt Vocab'))\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('corpus_counts_' + corpus + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get metrics on the bible corpus\n",
    "training_corpus = 'bible'\n",
    "bible_corpus_counter = corpus_counts(training_corpus)\n",
    "bible_corpus_table = tabulate_corpus_counts(bible_corpus_counter)\n",
    "print_corpus_counts(bible_corpus_table)\n",
    "bar_chart_corpus_counts(bible_corpus_table, 'Bible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get metrics on the europarl corpus\n",
    "training_corpus = 'europarl'\n",
    "europarl_corpus_counter = corpus_counts(training_corpus)\n",
    "europarl_corpus_table = tabulate_corpus_counts(europarl_corpus_counter)\n",
    "print_corpus_counts(europarl_corpus_table)\n",
    "bar_chart_corpus_counts(europarl_corpus_table, 'Europarl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get metrics on the wikipedia corpus\n",
    "## DON'T RUN FOR NOW - causing out of memory errors due to the size the corpus has grown to\n",
    "\n",
    "# training_corpus = 'wikipedia'\n",
    "# wikipedia_corpus_counter = corpus_counts(training_corpus)\n",
    "# wikipedia_corpus_table = tabulate_corpus_counts(wikipedia_corpus_counter)\n",
    "# print_corpus_counts(wikipedia_corpus_table)\n",
    "# bar_chart_corpus_counts(wikipedia_corpus_table, 'Wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get metrics on the bible-wikipedia enriched corpus\n",
    "# training_corpus = 'bible'\n",
    "# enrich_corpus = 'wikipedia'\n",
    "# enrich_langs = ['ar','en','es','fi','fr','he','hu','pt','tr']\n",
    "# bible_wikipedia_corpus_counter = corpus_counts(training_corpus, enrich_corpus)\n",
    "# bible_wikipedia_corpus_table = tabulate_corpus_counts(bible_wikipedia_corpus_counter)\n",
    "# unenriched_counts = dict()\n",
    "# for key, lang in bible_corpus_table.items():\n",
    "#     if key in enrich_langs:\n",
    "#         unenriched_counts[key] = lang\n",
    "# enriched_counts = dict()\n",
    "# for key, lang in bible_wikipedia_corpus_table.items():\n",
    "#     if key in enrich_langs:\n",
    "#         enriched_counts[key] = lang\n",
    "# print_corpus_counts(unenriched_counts, enriched_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Benchmark Analysis ###\n",
    "\n",
    "Identify what proportion of the source & target benchmark words are missing from the learnt vocabulary for each training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def benchmark_counts(bmk, source, target):\n",
    "    \n",
    "    base_path = './eval_data'\n",
    "    s_path = base_path + '/' + bmk + '/test.' + source\n",
    "    t_path = base_path + '/' + bmk + '/test.' + target\n",
    "    s_sents=[l.strip().split(\">\")[1].split(\"<\")[0].split() for l in codecs.open(s_path,'r',\"utf8\",errors='ignore').readlines()]\n",
    "    t_sents=[l.strip().split(\">\")[1].split(\"<\")[0].split() for l in codecs.open(t_path,'r',\"utf8\",errors='ignore').readlines()]\n",
    "    \n",
    "    s_vocab = Counter()\n",
    "    t_vocab = Counter()\n",
    "    \n",
    "    s_word_count = 0\n",
    "    t_word_count = 0\n",
    "    \n",
    "    for s_sent in s_sents:\n",
    "        s_word_count += len(s_sent)\n",
    "        s_vocab.update(Counter(s_sent))\n",
    "    \n",
    "    for t_sent in t_sents:\n",
    "        t_word_count += len(t_sent)\n",
    "        t_vocab.update(Counter(t_sent))\n",
    "    \n",
    "    bmk_counts = [len(s_sents), len(t_sents), s_word_count, t_word_count, \n",
    "                  len(s_vocab.keys()), len(t_vocab.keys()), s_vocab, t_vocab]\n",
    "    \n",
    "    return bmk_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bmk_counts = dict()\n",
    "bmk_counts['cakmak-en-tr'] = benchmark_counts('cakmak','en','tr')\n",
    "bmk_counts['holmqvist-en-sv'] = benchmark_counts('holmqvist','en','sv')\n",
    "bmk_counts['mihalcea-en-ro'] = benchmark_counts('mihalcea','en','ro')\n",
    "bmk_counts['lambert-en-es'] = benchmark_counts('lambert','en','es')\n",
    "bmk_counts['hansards-en-fr'] = benchmark_counts('hansards','en','fr')\n",
    "bmk_counts['graca-en-fr'] = benchmark_counts('graca/enfr','en','fr')\n",
    "bmk_counts['graca-en-es'] = benchmark_counts('graca/enes','en','es')\n",
    "bmk_counts['graca-en-pt'] = benchmark_counts('graca/enpt','en','pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_corpus_bmk_counts(bmk_counter, corpus_counter, source, target):\n",
    "    \n",
    "    comparison_stats = []\n",
    "    \n",
    "    try:\n",
    "        s_corpus_counter = corpus_counter[source]\n",
    "        t_corpus_counter = corpus_counter[target]\n",
    "\n",
    "        s_corpus_vocab = s_corpus_counter[3]\n",
    "        s_corpus_learnt = s_corpus_counter[5]\n",
    "        s_bmk_vocab = bmk_counter[6]\n",
    "\n",
    "        s_vocab_matches = 0\n",
    "        s_learnt_matches = 0\n",
    "        for key, value in s_bmk_vocab.items():\n",
    "            if s_corpus_vocab[key]:\n",
    "                s_vocab_matches += 1\n",
    "            if key in s_corpus_learnt:\n",
    "                s_learnt_matches += 1\n",
    "        s_vocab_match_pc = round((s_vocab_matches/len(s_bmk_vocab.items()))*100, 1)\n",
    "        s_learnt_match_pc = round((s_learnt_matches/len(s_bmk_vocab.items()))*100, 1)\n",
    "\n",
    "        t_corpus_vocab = t_corpus_counter[3]\n",
    "        t_corpus_learnt = t_corpus_counter[5]\n",
    "        t_bmk_vocab = bmk_counter[7]\n",
    "\n",
    "        t_vocab_matches = 0\n",
    "        t_learnt_matches = 0\n",
    "        for key, value in t_bmk_vocab.items():\n",
    "            if t_corpus_vocab[key]:\n",
    "                t_vocab_matches += 1\n",
    "            if key in t_corpus_learnt:\n",
    "                t_learnt_matches += 1\n",
    "        t_vocab_match_pc = round((t_vocab_matches/len(t_bmk_vocab.items()))*100, 1)\n",
    "        t_learnt_match_pc = round((t_learnt_matches/len(t_bmk_vocab.items()))*100, 1)\n",
    "\n",
    "        comparison_stats = [s_vocab_matches, s_learnt_matches, t_vocab_matches, t_learnt_matches,\n",
    "                           s_vocab_match_pc, s_learnt_match_pc, t_vocab_match_pc, t_learnt_match_pc]\n",
    "\n",
    "        return comparison_stats\n",
    "    except KeyError:\n",
    "        #print('Source or target language not available in corpus')\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_bmk_corpus_comparison(bmk_corpus_comparison):\n",
    "    print('bmk/corpus\\t\\ts_voc%\\ts_lnt%\\tt_voc%\\tt_lnt%')\n",
    "    print('==============\\t\\t======\\t======\\t======\\t======')\n",
    "    for key, value in sorted(bmk_corpus_comparison.items()):\n",
    "        print(key + '\\t' + str(value[4]) + '\\t' + str(value[5]) + '\\t' + str(value[6]) + '\\t' + str(value[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_all_bmks(bmks):\n",
    "    bmk_corpus_comparison = dict()\n",
    "    for key,value in bmks.items():\n",
    "        bible_comparison = compare_corpus_bmk_counts(value, bible_corpus_counter, key[-5:-3], key[-2:])\n",
    "        if bible_comparison is not None:\n",
    "            bmk_corpus_comparison[key + '-bible'] = bible_comparison\n",
    "        europarl_comparison = compare_corpus_bmk_counts(value, europarl_corpus_counter, key[-5:-3], key[-2:])\n",
    "        if europarl_comparison is not None:\n",
    "            bmk_corpus_comparison[key + '-europl'] = europarl_comparison\n",
    "#         bible_wikipedia_comparison = compare_corpus_bmk_counts(value, bible_wikipedia_corpus_counter, key[-5:-3], key[-2:])\n",
    "#         if bible_wikipedia_comparison is not None:\n",
    "#             bmk_corpus_comparison[key + '-bib-wik'] = bible_wikipedia_comparison\n",
    "    return bmk_corpus_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Vocabulary Word Percentages - Alignment Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_bmk_corpus_comparison(compare_all_bmks(bmk_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def wiktionary_counts(source, target):\n",
    "    base_path = './eval_data/wiktionary'\n",
    "    path = base_path + '/' + source + '-' + target + '-' + source + 'wiktionary.txt'\n",
    "    \n",
    "    BX = [(l.split(\"|||\")[-1].strip(), l.split(\"|||\")[0].strip()) for l in codecs.open(path, encoding='utf8').readlines()]\n",
    "    \n",
    "    s_vocab = set()\n",
    "    t_vocab = set()\n",
    "    \n",
    "    s_word_count = 0\n",
    "    t_word_count = 0\n",
    "    identical_count = 0\n",
    "    identical_capitalised_count = 0\n",
    "    \n",
    "    translit_u_filename = './eval_data/wiktionary/translit-upper-' + source + '-' + target + '.txt'\n",
    "    translit_l_filename = './eval_data/wiktionary/translit-lower-' + source + '-' + target + '.txt'\n",
    "    \n",
    "    # Clear down any previous version of the output files\n",
    "    if os.path.isfile(translit_u_filename):\n",
    "        os.remove(translit_u_filename)\n",
    "    if os.path.isfile(translit_l_filename):\n",
    "        os.remove(translit_l_filename)\n",
    "\n",
    "    with codecs.open(translit_u_filename, 'a', encoding='utf-8', errors='replace') as translit_u_file:\n",
    "        with codecs.open(translit_l_filename, 'a', encoding='utf-8', errors='replace') as translit_l_file:\n",
    "    \n",
    "            for s,t in BX:\n",
    "                s_vocab.add(s)\n",
    "                t_vocab.add(t)\n",
    "                if s == t:\n",
    "                    identical_count += 1\n",
    "                    if s[0].isupper():\n",
    "                        translit_u_file.write(s + ' ||| ' + t + '\\n')\n",
    "                        identical_capitalised_count += 1\n",
    "                    else:\n",
    "                        translit_l_file.write(s + ' ||| ' + t + '\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    identical_pc = identical_count * 100 / len(BX)\n",
    "    identical_capitalised_pc = identical_capitalised_count * 100 / len(BX)\n",
    "    \n",
    "    counts = [len(BX), s_vocab, t_vocab, identical_count, identical_pc, \n",
    "              identical_capitalised_count, identical_capitalised_pc]\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiknry_counts = dict()\n",
    "wiknry_counts['en-ar'] = wiktionary_counts('en', 'ar')\n",
    "wiknry_counts['en-es'] = wiktionary_counts('en', 'es')\n",
    "wiknry_counts['en-fi'] = wiktionary_counts('en', 'fi')\n",
    "wiknry_counts['en-fr'] = wiktionary_counts('en', 'fr')\n",
    "wiknry_counts['en-he'] = wiktionary_counts('en', 'he')\n",
    "wiknry_counts['en-hu'] = wiktionary_counts('en', 'hu')\n",
    "wiknry_counts['en-pt'] = wiktionary_counts('en', 'pt')\n",
    "wiknry_counts['en-tr'] = wiktionary_counts('en', 'tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_wiktionary_counts(counts, lang_pairs):\n",
    "    for lang_pair in lang_pairs:\n",
    "        this_count = counts[lang_pair[0] + '-' + lang_pair[1]]\n",
    "        print(lang_pair[0] + '-' + lang_pair[1] + ': ' + str(this_count[0]) + \n",
    "              '\\t transliterations: ' + str(this_count[3]) + ' (' + str(round(this_count[4],1)) + '%)' + \n",
    "             '\\t of which capitalized: ' + str(this_count[5]) + ' (' + str(round(this_count[6],1)) + '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lang_pairs = [('en','ar'), ('en','es'), ('en','fi'), ('en','fr'), ('en','he'), ('en','hu'), ('en','pt'), ('en','tr')]\n",
    "print_wiktionary_counts(wiknry_counts, lang_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_corpus_wiknry_counts(wiknry_counter, corpus_counter, source, target):\n",
    "    \n",
    "    comparison_stats = []\n",
    "    \n",
    "    try:\n",
    "        wiknry_word_count = wiknry_counter[0]\n",
    "        \n",
    "        s_corpus_counter = corpus_counter[source]\n",
    "        t_corpus_counter = corpus_counter[target]\n",
    "\n",
    "        s_corpus_vocab = s_corpus_counter[3]\n",
    "        s_corpus_learnt = s_corpus_counter[5]\n",
    "        s_wiknry_vocab = wiknry_counter[1]\n",
    "\n",
    "        s_vocab_matches = 0\n",
    "        s_learnt_matches = 0\n",
    "        for word in s_wiknry_vocab:\n",
    "            if s_corpus_vocab[word]:\n",
    "                s_vocab_matches += 1\n",
    "            if word in s_corpus_learnt:\n",
    "                s_learnt_matches += 1\n",
    "        s_vocab_match_pc = round((s_vocab_matches/len(s_wiknry_vocab))*100, 1)\n",
    "        s_learnt_match_pc = round((s_learnt_matches/len(s_wiknry_vocab))*100, 1)\n",
    "\n",
    "        t_corpus_vocab = t_corpus_counter[3]\n",
    "        t_corpus_learnt = t_corpus_counter[5]\n",
    "        t_wiknry_vocab = wiknry_counter[2]\n",
    "\n",
    "        t_vocab_matches = 0\n",
    "        t_learnt_matches = 0\n",
    "        for word in t_wiknry_vocab:\n",
    "            if t_corpus_vocab[word]:\n",
    "                t_vocab_matches += 1\n",
    "            if word in t_corpus_learnt:\n",
    "                t_learnt_matches += 1\n",
    "        t_vocab_match_pc = round((t_vocab_matches/len(t_wiknry_vocab))*100, 1)\n",
    "        t_learnt_match_pc = round((t_learnt_matches/len(t_wiknry_vocab))*100, 1)\n",
    "\n",
    "        comparison_stats = [s_vocab_matches, s_learnt_matches, t_vocab_matches, t_learnt_matches,\n",
    "                           s_vocab_match_pc, s_learnt_match_pc, t_vocab_match_pc, t_learnt_match_pc]\n",
    "\n",
    "        return comparison_stats\n",
    "    except KeyError:\n",
    "        #print('Source or target language not available in corpus')\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_wiknry_corpus_comparison(wiknry_corpus_comparison):\n",
    "    print('lang/corpus\\ts_voc%\\ts_lnt%\\tt_voc%\\tt_lnt%')\n",
    "    print('==============\\t======\\t======\\t======\\t======')\n",
    "    for key, value in sorted(wiknry_corpus_comparison.items()):\n",
    "        print(key + '\\t' + str(value[4]) + '\\t' + str(value[5]) + '\\t' + str(value[6]) + '\\t' + str(value[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_wiknry_vocab(counts):\n",
    "    wiknry_corpus_comparison = dict()\n",
    "    for key,value in counts.items():\n",
    "        bible_comparison = compare_corpus_wiknry_counts(value, bible_corpus_counter, key[-5:-3], key[-2:])\n",
    "        if bible_comparison is not None:\n",
    "            wiknry_corpus_comparison[key + '-bible'] = bible_comparison\n",
    "        europarl_comparison = compare_corpus_wiknry_counts(value, europarl_corpus_counter, key[-5:-3], key[-2:])\n",
    "        if europarl_comparison is not None:\n",
    "            wiknry_corpus_comparison[key + '-europarl'] = europarl_comparison\n",
    "#         bible_wikipedia_comparison = compare_corpus_wiknry_counts(value, bible_wikipedia_corpus_counter, key[-5:-3], key[-2:])\n",
    "#         if bible_wikipedia_comparison is not None:\n",
    "#             wiknry_corpus_comparison[key + '-bib-wik'] = bible_wikipedia_comparison\n",
    "    return wiknry_corpus_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Vocabulary Word Percentages - Wiktionary Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_wiknry_corpus_comparison(compare_wiknry_vocab(wiknry_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
