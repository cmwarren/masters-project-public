{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_vocab_counts(training_corpus):\n",
    "    corpus_vocab = dict()\n",
    "    training_word_counts_file = './' + training_corpus + '/counts.words.vocab'\n",
    "    with codecs.open(training_word_counts_file, 'r', encoding='utf-8', errors='replace') as counts_file:\n",
    "        count_entries = [line.strip().split(' ') for line in counts_file]\n",
    "        for count_entry in count_entries:\n",
    "            corpus_vocab[count_entry[0]] = (int(count_entry[1]), 0)\n",
    "    return corpus_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collate_training_enrich_vocab_counts(training_corpus, enrich_corpus):\n",
    "    \n",
    "    corpus_vocab = get_training_vocab_counts(training_corpus)\n",
    "\n",
    "    enrich_word_counts_file = './' + enrich_corpus + '/counts.words.vocab'\n",
    "    with codecs.open(enrich_word_counts_file, 'r', encoding='utf-8', errors='replace') as counts_file:\n",
    "        count_entries = [line.strip().split(' ') for line in counts_file]\n",
    "        for count_entry in count_entries:\n",
    "            if count_entry[0] in corpus_vocab:\n",
    "                count_tuple = corpus_vocab[count_entry[0]]\n",
    "                if (len(count_tuple) == 1):\n",
    "                    corpus_vocab[count_entry[0]] = (count_tuple[0], int(count_entry[1]))\n",
    "                elif (len(count_tuple) == 2):\n",
    "                    corpus_vocab[count_entry[0]] = (count_tuple[0], count_tuple[1] + int(count_entry[1]))\n",
    "            else:\n",
    "                corpus_vocab[count_entry[0]] = (0, int(count_entry[1]))\n",
    "    return corpus_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_subset_corpus_freq_dist(eval_subset_filename, lang_pair, corpus_vocab, s_freq_dist=None, t_freq_dist=None):\n",
    "    if s_freq_dist is None:\n",
    "        s_freq_dist = dict()\n",
    "    if t_freq_dist is None:\n",
    "        t_freq_dist = dict()\n",
    "    BX = [(l.split(\"|||\")[-1].strip(), l.split(\"|||\")[0].strip()) \n",
    "          for l in codecs.open(eval_subset_filename, encoding='utf-8', errors='replace').readlines()]\n",
    "    for t, s in BX:\n",
    "        s_prefixed = lang_pair[0] + '0_' + s\n",
    "        if s_prefixed in corpus_vocab:\n",
    "            s_freq_dist[s_prefixed] = corpus_vocab[s_prefixed]\n",
    "        t_prefixed = lang_pair[1] + '0_' + t\n",
    "        if t_prefixed in corpus_vocab:\n",
    "            t_freq_dist[t_prefixed] = corpus_vocab[t_prefixed]\n",
    "        \n",
    "    return [s_freq_dist, t_freq_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_set_corpus_freq_dist(eval_dir, corpus_vocab, lang_pairs, eval_subsets, b_merge_subsets=True):\n",
    "    eval_path = './' + training_corpus + '/' + eval_dir + '/'\n",
    "    freq_dists = dict()\n",
    "    for lang_pair in lang_pairs:\n",
    "        s_key = lang_pair[0] + '-' + lang_pair[1]\n",
    "        t_key = lang_pair[1] + '-' + lang_pair[0]\n",
    "        for eval_subset in eval_subsets:\n",
    "            if b_merge_subsets:\n",
    "                if s_key in freq_dists:\n",
    "                    s_freq_dist = freq_dists[s_key]\n",
    "                else:\n",
    "                    s_freq_dist = None\n",
    "                if t_key in freq_dists:\n",
    "                    t_freq_dist = freq_dists[t_key]\n",
    "                else:\n",
    "                    t_freq_dist = None\n",
    "            else:\n",
    "                s_key = eval_subset + '-' + s_key\n",
    "                t_key = eval_subset + '-' + t_key\n",
    "                s_freq_dist = None\n",
    "                t_freq_dist = None\n",
    "            eval_file = eval_path + eval_subset + '-' + lang_pair[0] + '-' + lang_pair[1] + '.txt'    \n",
    "            [s_freq_dist, t_freq_dist] = eval_subset_corpus_freq_dist(eval_file, lang_pair, corpus_vocab, s_freq_dist, s_freq_dist)\n",
    "            freq_dists[s_key] = s_freq_dist\n",
    "            freq_dists[t_key] = t_freq_dist\n",
    "    return freq_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_corpus_freq_dist(corpus_vocab, langs):\n",
    "    freq_dists = dict()\n",
    "    for lang in langs:\n",
    "        freq_dist = dict()\n",
    "        for corpus_item in corpus_vocab.items():\n",
    "            if corpus_item[0][0:2] == lang:\n",
    "                freq_dist[corpus_item[0]] = corpus_item[1]\n",
    "        freq_dists[lang] = freq_dist\n",
    "    return freq_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_set_baseline_corpus_freq_dist(eval_dir, corpus_vocab, lang_pairs):\n",
    "    eval_path = './' + training_corpus + '/'  + eval_dir + '/'\n",
    "    freq_dists = dict()\n",
    "    for lang_pair in lang_pairs:\n",
    "        eval_file = eval_path + 'inv' + '-' + lang_pair[0] + '-' + lang_pair[1] + '.txt'     \n",
    "        print('eval_file: ' + eval_file)\n",
    "        [s_freq_dist, t_freq_dist] = eval_subset_corpus_freq_dist(eval_file, lang_pair, corpus_vocab)\n",
    "        freq_dists[lang_pair[0] + '-' + lang_pair[1]] = s_freq_dist\n",
    "        freq_dists[lang_pair[1] + '-' + lang_pair[0]] = t_freq_dist\n",
    "    return freq_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_dists_to_data_frames(freq_dists):  \n",
    "    df_freq_dists = dict()\n",
    "    for lang_key, freq_dist in freq_dists.items():\n",
    "        df_freq_dist = pd.DataFrame(freq_dist, index=['train','enrich']).transpose()\n",
    "        df_freq_dist['total'] = df_freq_dist['train'] + df_freq_dist['enrich']\n",
    "        df_freq_dists[lang_key] = df_freq_dist\n",
    "    return df_freq_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_freq_dists(freq_dists):\n",
    "    for lang_key, freq_dist in freq_dists.items():\n",
    "        \n",
    "        sorted_keys = sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "        train_freqs = [freq_dist[word_key][0] for word_key in sorted_keys]\n",
    "        enrich_freqs = [freq_dist[word_key][1] for word_key in sorted_keys]\n",
    "        print 'Vocabulary size: ' + str(len(train_freqs))\n",
    "        ind = np.arange(len(train_freqs))\n",
    "\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        plt.autoscale(enable=True)\n",
    "\n",
    "        axes = fig.get_axes()\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(xmax=20000)\n",
    "            ax.set_ylim(1,10**5)\n",
    "\n",
    "\n",
    "        p1 = plt.bar(ind, train_freqs, color='b', log=True)\n",
    "        p2 = plt.bar(ind, enrich_freqs, color='r', bottom=train_freqs, log=True)\n",
    "\n",
    "\n",
    "        plt.xlabel('Word Entries')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(r'Frequency Distribution: ' + lang_key)\n",
    "        plt.legend((p1[0], p2[0]), (training_corpus, enrich_corpus))\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "        fig.savefig('enrich_freq_dist/' + lang_key + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_baseline_freq_dists(freq_dists, color, eval_subset):\n",
    "    for lang_key, freq_dist in freq_dists.items():\n",
    "        \n",
    "        sorted_keys = sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "        train_freqs = [freq_dist[word_key][0] for word_key in sorted_keys]\n",
    "        print 'Vocabulary size: ' + str(len(train_freqs))\n",
    "        ind = np.arange(len(train_freqs))\n",
    "\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        plt.autoscale(enable=True)\n",
    "\n",
    "        axes = fig.get_axes()\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(xmax=20000)\n",
    "            ax.set_ylim(1,10**5)\n",
    "\n",
    "        p1 = plt.bar(ind, train_freqs, color=color, log=True)\n",
    "\n",
    "\n",
    "        plt.xlabel('Word Entries')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(r'Frequency Distribution: ' + eval_subset + lang_key)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "        fig.savefig('baseline_freq_dist/' + eval_subset + lang_key + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
