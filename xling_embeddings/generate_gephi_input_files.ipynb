{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import sys\n",
    "sys.path.append('hyperwords')\n",
    "from representations.embedding import Embedding\n",
    "from representations.matrix_serializer import save_vocabulary\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ./frequency_lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_corpus = 'bible'        \n",
    "enrich_corpus = 'twitter'\n",
    "baseline_output_path = './' + training_corpus + '/'\n",
    "enriched_output_path = './' + training_corpus + '-' + enrich_corpus + '/'\n",
    "baseline_sgns_dir = 'sgns'\n",
    "enriched_sgns_dir = 'sgns_enriched_0'\n",
    "eval_dir = 'wiktionary-eval'\n",
    "lang_pairs = [('en','fr'),('en','es'),('en','fi'),('fr','en'),('es','en'),('fi','en')]\n",
    "eval_subsets = ['inv', 'oov']\n",
    "lang_codes = list(set([pair[0] for pair in lang_pairs] + [pair[1] for pair in lang_pairs]))\n",
    "baseline_sim_dir = baseline_sgns_dir + '/similarity-matrices/'\n",
    "baseline_sim_set_filepath = baseline_sim_dir + '-'.join(lang_codes) + '.vecs'\n",
    "baseline_sim_csv_filepath = baseline_sim_dir + '-'.join(lang_codes)\n",
    "enriched_sim_dir = enriched_sgns_dir + '/similarity-matrices/'\n",
    "enriched_sim_set_filepath = enriched_sim_dir + '-'.join(lang_codes) + '.vecs'\n",
    "enriched_sim_csv_filepath = enriched_sim_dir + '-'.join(lang_codes)\n",
    "similarity_threshold = 0.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(baseline_output_path + baseline_sim_dir):\n",
    "    os.mkdir(baseline_output_path + baseline_sim_dir) \n",
    "if not os.path.exists(enriched_output_path + enriched_sim_dir):\n",
    "    os.mkdir(enriched_output_path + enriched_sim_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vectors(path):\n",
    "    vectors = {}\n",
    "    with open(path) as f:\n",
    "        first_line = False\n",
    "        for line in f:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                continue\n",
    "            tokens = line.strip().split(' ')\n",
    "            vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2numpy_nonewline(path):\n",
    "    \n",
    "    matrix = read_vectors(path)\n",
    "    iw = sorted(matrix.keys())\n",
    "    \n",
    "    new_matrix = np.zeros(shape=(len(iw), len(matrix[iw[0]])), dtype=np.float32)\n",
    "    for i, word in enumerate(iw):\n",
    "        if word in matrix:\n",
    "            new_matrix[i, :] = matrix[word]\n",
    "\n",
    "    npy_file = path + '.npy'\n",
    "    vocab_file = path + '.vocab'\n",
    "    \n",
    "    np.save(npy_file, new_matrix)\n",
    "    save_vocabulary(vocab_file, iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi2subset(path, outpath, lang_codes, vocab_subset):\n",
    "    subset = list()\n",
    "\n",
    "    with codecs.open(path, encoding='utf-8', errors='replace') as fin:\n",
    "        for line in fin:\n",
    "            token = line.split()[0]\n",
    "            language = token[:2]\n",
    "            new_line = token + line.strip()[len(token):]\n",
    "            if language in lang_codes and token in vocab_subset:\n",
    "                subset.append(new_line)\n",
    "\n",
    "    with codecs.open(outpath, 'w', encoding='utf-8', errors='replace') as fout:\n",
    "        fout.write('\\n'.join(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_lang_vecs(lang_codes, path, sim_set_filename, vocab_subset):\n",
    "    \n",
    "    sgns_words_path = path + '/words'\n",
    "    \n",
    "    multi2subset(sgns_words_path, sim_set_filename, lang_codes, vocab_subset)\n",
    "    \n",
    "    text2numpy_nonewline(sim_set_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_matrix(sim_set_filename):\n",
    "        \n",
    "    Es = Embedding(sim_set_filename, True)\n",
    "    Et = Embedding(sim_set_filename, True)\n",
    "    \n",
    "    Es_sparse = sparse.csr_matrix(Es.m)\n",
    "    Et_sparse = sparse.csr_matrix(Et.m)\n",
    "    \n",
    "    score_matrix = Es_sparse.dot(Et_sparse.T)\n",
    "    \n",
    "    return [score_matrix, Es.iw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count_tuple(corpus_vocab, word):\n",
    "    word = word.decode('utf-8')\n",
    "    if (word) in corpus_vocab:\n",
    "        return corpus_vocab[word]\n",
    "    else:\n",
    "        print word\n",
    "        return (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_matrix_to_edges(df_similarity_matrix, word_index):    \n",
    "    # Gephi input seems to require a flat tabular input of nodes and edges?\n",
    "    df_edge_table = pd.DataFrame(columns=['source','target','similarity'])\n",
    "    \n",
    "    for src_word in word_index:\n",
    "        for trg_word in word_index:\n",
    "            if src_word != trg_word and trg_word not in df_edge_table['source'].unique():\n",
    "                sim = df_similarity_matrix.loc[src_word][trg_word]\n",
    "                if not np.isnan(sim):\n",
    "                    df_edge_table = df_edge_table.append(\n",
    "                        {'source':src_word, 'target':trg_word, 'similarity':sim} ,ignore_index=True)\n",
    "    \n",
    "    df_edge_table['Type'] = 'Undirected'\n",
    "    return df_edge_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_index_to_nodes(word_index, corpus_vocab):\n",
    "    df_node_table = pd.DataFrame(columns=['Id','Label','language','from_corpus','train_count','enrich_count'])\n",
    "    for word in word_index:\n",
    "    \n",
    "        if word not in df_node_table['Id'].unique():\n",
    "            count_tuple = get_count_tuple(corpus_vocab, word)\n",
    "            df_node_table = df_node_table.append(\n",
    "                {'Id': word, 'Label': word, 'language': word[0:2], \n",
    "                 'from_corpus': 'train' if count_tuple[0] > 0 else 'enrich',\n",
    "                 'train_count': count_tuple[0], 'enrich_count': count_tuple[1]}, ignore_index=True)\n",
    "    return df_node_table   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build training+enrich corpus vocab counts & the frequency distributions of evaluation words,\n",
    "# so that we can extract a smaller subset of interesting words to investigate in Gephi\n",
    "corpus_vocab = collate_training_enrich_vocab_counts(training_corpus, enrich_corpus)\n",
    "freq_dists = eval_set_corpus_freq_dist(eval_dir, corpus_vocab, lang_pairs, eval_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discard words with frequency>1000 - likely to be stop words, not very interesting\n",
    "# Limit the number of words per language to avoid memory issues computing similarity matrix\n",
    "df_freq_dists = freq_dists_to_data_frames(freq_dists)\n",
    "vocab_subset = set()\n",
    "for key,df_freq_dist in df_freq_dists.items():\n",
    "    if key.startswith('oov') and key.endswith('src'):\n",
    "        df_subset = df_freq_dist.query('total < 1000').sort_values(['train'], ascending=False).head(200)\n",
    "        vocab_subset.update(df_subset.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_lang_vecs(lang_codes, baseline_output_path + baseline_sgns_dir, \n",
    "                  baseline_output_path + baseline_sim_set_filepath, vocab_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[baseline_score_matrix, baseline_word_index] = similarity_matrix(baseline_output_path + baseline_sim_set_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_baseline_sim_matrix = pd.DataFrame(baseline_score_matrix.toarray(), \n",
    "                                      index=baseline_word_index, columns=baseline_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_baseline_sim_matrix = df_baseline_sim_matrix[df_baseline_sim_matrix.gt(similarity_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_baseline_node_table = word_index_to_nodes(baseline_word_index, corpus_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_baseline_edge_table = sim_matrix_to_edges(df_baseline_sim_matrix, baseline_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_baseline_node_table.to_csv(baseline_output_path + baseline_sim_csv_filepath + '-node.csv', \n",
    "                              encoding='utf-8', header=True, index=True)\n",
    "df_baseline_edge_table.to_csv(baseline_output_path + baseline_sim_csv_filepath + '-edge.csv', \n",
    "                              encoding='utf-8', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_node_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_edge_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_lang_vecs(lang_codes, enriched_output_path + enriched_sgns_dir, \n",
    "                  enriched_output_path + enriched_sim_set_filepath, vocab_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[enriched_score_matrix, enriched_word_index] = similarity_matrix(enriched_output_path + enriched_sim_set_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_enriched_sim_matrix = pd.DataFrame(enriched_score_matrix.toarray(), \n",
    "                                      index=enriched_word_index, columns=enriched_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_enriched_sim_matrix = df_enriched_sim_matrix[df_enriched_sim_matrix.gt(similarity_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_enriched_node_table = word_index_to_nodes(enriched_word_index, corpus_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_enriched_edge_table = sim_matrix_to_edges(df_enriched_sim_matrix, enriched_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_enriched_node_table.to_csv(enriched_output_path + enriched_sim_csv_filepath + '-node.csv', \n",
    "                              encoding='utf-8', header=True, index=True)\n",
    "df_enriched_edge_table.to_csv(enriched_output_path + enriched_sim_csv_filepath + '-edge.csv', \n",
    "                              encoding='utf-8', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_node_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_edge_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
